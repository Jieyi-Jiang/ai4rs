import math
from functools import lru_cache
from typing import Dict, Optional, Tuple

import torch
from mmengine.dist import get_world_size
from mmengine.logging import print_log
from torch import Tensor, nn
# from mmdet.models.detectors import DINO
# from mmdet.models.detectors import RotatedDINO
from .rotated_dino import RotatedDINO
from mmdet.models.detectors.deformable_detr import DeformableDETR, MultiScaleDeformableAttention
from mmdet.structures import OptSampleList
# from projects.rtdetr_o.rtdetr.rtdetr_layers import RTDETRHybridEncoder
from projects.rtdetr_o.rtdetr.rotated_rtdetr_layers import RTDETRHybridEncoder, RotatedRTDETRTransformerDecoder
from mmrotate.registry import MODELS
from .utils import assert_no_nan_inf, assert_no_nan

@MODELS.register_module()
class RotatedRTDETR(RotatedDINO):
    r"""Implementation of `DETRs Beat YOLOs on Real-time Object Detection
    <https://arxiv.org/abs/2304.08069>`_

    Code is modified from the `official github repo
    <https://github.com/lyuwenyu/RT-DETR>`_.

    Args:
        use_syncbn (bool): Whether to use SyncBatchNorm. Defaults to True.
    """

    def __init__(self,
                 *args,
                 eval_idx: int = -1,
                 spatial_shapes: Optional[Tuple[Tuple[int, int]]] = None,
                 use_syncbn: bool = True,
                 **kwargs) -> None:
        kwargs['encoder']['spatial_shapes'] = spatial_shapes
        super().__init__(*args, **kwargs)
        self.eval_idx = eval_idx

        if spatial_shapes is not None:
            spatial_shapes = tuple(map(tuple, spatial_shapes))
            proposals, proposals_valid = self.gen_proposals(spatial_shapes)
            self.register_buffer('proposals', proposals, persistent=False)
            self.register_buffer(
                'proposals_valid', proposals_valid, persistent=False)
        else:
            self.proposals, self.proposals_valid = None, None

        # TODO: Waiting for mmengine support
        if use_syncbn and get_world_size() > 1:
            torch.nn.SyncBatchNorm.convert_sync_batchnorm(self)
            print_log('Using SyncBatchNorm()', 'current')

    def _init_layers(self) -> None:
        """Initialize layers except for backbone, neck and bbox_head."""
        self.encoder = RTDETRHybridEncoder(**self.encoder)
        self.decoder = RotatedRTDETRTransformerDecoder(**self.decoder)
        self.embed_dims = self.decoder.embed_dims
        self.memory_trans_fc = nn.Linear(self.embed_dims, self.embed_dims)
        self.memory_trans_norm = nn.LayerNorm(self.embed_dims)

    def init_weights(self) -> None:
        """Initialize weights for Transformer and other components."""
        super(DeformableDETR, self).init_weights()
        for p in self.decoder.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
            elif isinstance(m, nn.MultiheadAttention):
                nn.init.kaiming_uniform_(m.out_proj.weight, a=math.sqrt(5))
        nn.init.xavier_uniform_(self.memory_trans_fc.weight)

    def pre_transformer(
            self,
            mlvl_feats: Tuple[Tensor],
            batch_data_samples: OptSampleList = None) -> Tuple[Dict, Dict]:
        """Process image features before feeding them to the transformer.

        The forward procedure of the transformer is defined as:
        'pre_transformer' -> 'encoder' -> 'pre_decoder' -> 'decoder'
        More details can be found at `TransformerDetector.forward_transformer`
        in `mmdet/detector/base_detr.py`.

        Args:
            mlvl_feats (tuple[Tensor]): Multi-level features that may have
                different resolutions, output from neck. Each feature has
                shape (bs, dim, h_lvl, w_lvl), where 'lvl' means 'layer'.
            batch_data_samples (list[:obj:`DetDataSample`], optional): The
                batch data samples. It usually includes information such
                as `gt_instance` or `gt_panoptic_seg` or `gt_sem_seg`.
                Defaults to None.

        Returns:
            tuple[dict]: The first dict contains the inputs of encoder and the
            second dict contains the inputs of decoder.

            - encoder_inputs_dict (dict): The keyword args dictionary of
              `self.forward_encoder()`, which includes 'mlvl_feats' and
              'spatial_shapes'.
            - decoder_inputs_dict (dict): The keyword args dictionary of
              `self.forward_decoder()`, which includes 'spatial_shapes' and
              `level_start_index`.
        """
        spatial_shapes = []
        for feat in mlvl_feats:
            spatial_shape = torch._shape_as_tensor(feat)[2:].to(feat.device)
            spatial_shapes.append(spatial_shape)

        # (num_level, 2)
        spatial_shapes = torch.cat(spatial_shapes).view(-1, 2)
        level_start_index = torch.cat((
            spatial_shapes.new_zeros((1, )),  # (num_level)
            spatial_shapes.prod(1).cumsum(0)[:-1]))

        encoder_inputs_dict = dict(
            mlvl_feats=mlvl_feats, spatial_shapes=spatial_shapes)
        decoder_inputs_dict = dict(
            memory_mask=None,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=None)
        return encoder_inputs_dict, decoder_inputs_dict

    def forward_encoder(self, mlvl_feats: Tuple[Tensor],
                        spatial_shapes: Tensor) -> Dict:
        """Forward with Transformer encoder.

        The forward procedure of the transformer is defined as:
        'pre_transformer' -> 'encoder' -> 'pre_decoder' -> 'decoder'
        More details can be found at `TransformerDetector.forward_transformer`
        in `mmdet/detector/base_detr.py`.

        Args:
            mlvl_feats (tuple[Tensor]): Multi-level features that may have
                different resolutions, output from neck. Each feature has
                shape (bs, dim, h_lvl, w_lvl), where 'lvl' means 'layer'.
            spatial_shapes (Tensor): Spatial shapes of features in all levels,
                has shape (num_levels, 2), last dimension represents (h, w).

        Returns:
            dict: The output of the Transformer encoder, which includes
            `memory` and `spatial_shapes`.
        """
        mlvl_feats = self.encoder(mlvl_feats)

        feat_flatten = []
        for feat in mlvl_feats:
            batch_size, c, h, w = feat.shape
            # [bs, c, h_lvl, w_lvl] -> [bs, h_lvl*w_lvl, c]
            feat = feat.view(batch_size, c, -1).permute(0, 2, 1)
            feat_flatten.append(feat)

        # (bs, num_feat_points, dim)
        memory = torch.cat(feat_flatten, 1)

        encoder_outputs_dict = dict(
            memory=memory, memory_mask=None, spatial_shapes=spatial_shapes)
        return encoder_outputs_dict

    def pre_decoder(
        self,
        memory: Tensor,
        memory_mask: Tensor,
        spatial_shapes: Tensor,
        batch_data_samples: OptSampleList = None,
    ) -> Tuple[Dict]:
        """Prepare intermediate variables before entering Transformer decoder,
        such as `query`, `query_pos`, and `reference_points`.

        Args:
            memory (Tensor): The output embeddings of the Transformer encoder,
                has shape (bs, num_feat_points, dim).
            memory_mask (Tensor): ByteTensor, the padding mask of the memory,
                has shape (bs, num_feat_points). Will only be used when
                `as_two_stage` is `True`.
            spatial_shapes (Tensor): Spatial shapes of features in all levels.
                With shape (num_levels, 2), last dimension represents (h, w).
                Will only be used when `as_two_stage` is `True`.
            batch_data_samples (list[:obj:`DetDataSample`]): The batch
                data samples. It usually includes information such
                as `gt_instance` or `gt_panoptic_seg` or `gt_sem_seg`.
                Defaults to None.

        Returns:
            tuple[dict]: The decoder_inputs_dict and head_inputs_dict.

            - decoder_inputs_dict (dict): The keyword dictionary args of
              `self.forward_decoder()`, which includes 'query', 'memory',
              `reference_points`, and `dn_mask`. The reference points of
              decoder input here are 4D boxes, although it has `points`
              in its name.
            - head_inputs_dict (dict): The keyword dictionary args of the
              bbox_head functions, which includes `topk_score`, `topk_coords`,
              and `dn_meta` when `self.training` is `True`, else is empty.
        """
        bs, _, c = memory.shape
        cls_out_features = self.bbox_head.cls_branches[
            self.decoder.num_layers].out_features

        output_memory, output_proposals = self.gen_encoder_output_proposals(
            memory, memory_mask, spatial_shapes)
        enc_outputs_class = self.bbox_head.cls_branches[
            self.decoder.num_layers](
                output_memory)
        assert_no_nan(enc_outputs_class)

        # NOTE The DINO selects top-k proposals according to scores of
        # multi-class classification, while DeformDETR, where the input
        # is `enc_outputs_class[..., 0]` selects according to scores of
        # binary classification.
        topk_indices = torch.topk(
            enc_outputs_class.max(-1)[0], k=self.num_queries, dim=1)[1]

        query = torch.gather(output_memory, 1,
                             topk_indices.unsqueeze(-1).repeat(1, 1, c))
        topk_output_proposals = torch.gather(
            output_proposals, 1,
            topk_indices.unsqueeze(-1).repeat(1, 1, 5))
        assert_no_nan(topk_output_proposals)
        head_output = self.bbox_head.reg_branches[self.decoder.num_layers](query)
        assert_no_nan(head_output)
        topk_coords_unact = head_output + topk_output_proposals
        # assert_no_nan(topk_coords_unact)


        if self.training:
            topk_score = torch.gather(
                enc_outputs_class, 1,
                topk_indices.unsqueeze(-1).repeat(1, 1, cls_out_features))
            topk_coords = topk_coords_unact.sigmoid()
            topk_coords_unact = topk_coords_unact.detach()
            dn_label_query, dn_bbox_query, dn_mask, dn_meta = \
                self.dn_query_generator(batch_data_samples)
            query = query.detach()  # detach() is not used in DINO
            query = torch.cat([dn_label_query, query], dim=1)
            dn_bbox_query = dn_bbox_query.type_as(topk_coords_unact)
            reference_points = torch.cat([dn_bbox_query, topk_coords_unact],
                                         dim=1)
        else:
            topk_coords = topk_coords_unact.sigmoid()
            reference_points = topk_coords_unact
            dn_mask, dn_meta = None, None
        # NOTE To avoid inverse_sigmoid in decoder
        # reference_points = reference_points.sigmoid()

        assert_no_nan(reference_points) 
        decoder_inputs_dict = dict(
            query=query,
            memory=memory,
            reference_points=reference_points,
            dn_mask=dn_mask,
            cls_branches=self.bbox_head.cls_branches,
            eval_idx=self.eval_idx)
        # NOTE DINO calculates encoder losses on scores and coordinates
        # of selected top-k encoder queries, while DeformDETR is of all
        # encoder queries.
        assert_no_nan(topk_coords)
        head_inputs_dict = dict(
            enc_outputs_class=topk_score,
            enc_outputs_coord=topk_coords,
            dn_meta=dn_meta) if self.training else dict()
        return decoder_inputs_dict, head_inputs_dict

    def gen_encoder_output_proposals(
            self, memory: Tensor, memory_mask: Tensor,
            spatial_shapes: Tensor) -> Tuple[Tensor, Tensor]:
        assert memory_mask is None

        batch_size = memory.size(0)
        # print(self.proposals)
        if (not self.training and self.proposals is not None
                and self.proposals_valid is not None and batch_size == 1):
            output_proposals = self.proposals
            output_proposals_valid = self.proposals_valid
        else:
            spatial_shapes = tuple(map(tuple, spatial_shapes.tolist()))
            # print(f"spatial_shapes: {spatial_shapes}")
            output_proposals, output_proposals_valid = self.gen_proposals(
                spatial_shapes, batch_size, memory.device)
        # assert_no_nan_inf(output_proposals)
        # print(f'DEBUG >> spatial_shapes: {spatial_shapes}')
        # print(f"DEBUG >> output_proposals.shape: {output_proposals.shape}")
        # print(f"DEBUG >> memory.shape: {memory.shape}")
        output_memory = memory * output_proposals_valid.type_as(memory)
        output_memory = self.memory_trans_fc(output_memory)
        output_memory = self.memory_trans_norm(output_memory)
        # [bs, sum(hw), 2]
        return output_memory, output_proposals

    @staticmethod
    @lru_cache
    def gen_proposals(
            spatial_shapes: Tuple[Tuple[int, int]],
            batch_size: int = 1,
            device: Optional[str] = None,
            dtype: torch.dtype = torch.float32) -> Tuple[Tensor, Tensor]:
            
        proposals = []
        for lvl, HW in enumerate(spatial_shapes):
            # print(f'DEBUG >> lvl: {lvl}')
            # print(f'DEBUG >> HW: {HW}')
            # print(f"lvl: {lvl}")
            H, W = HW
            HW = torch.tensor(HW, dtype=torch.float32, device=device)
            scale = HW.unsqueeze(0).flip(dims=[0, 1]).view(1, 1, 1, 2)
            # grid_y, grid_x = torch.meshgrid(
            #     torch.linspace(
            #         0, H - 1, H, dtype=torch.float32, device=device),
            #     torch.linspace(
            #         0, W - 1, W, dtype=torch.float32, device=device))

            # 1. 生成归一化中心点 (cx, cy) ∈ (0, 1)
            grid_y, grid_x = torch.meshgrid(
                torch.linspace(0.5 / H, (H - 0.5) / H, H, dtype=dtype, device=device),
                torch.linspace(0.5 / W, (W - 0.5) / W, W, dtype=dtype, device=device),
                indexing='ij'
            )  # (H, W)
            assert_no_nan(grid_y)
            # grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)
            # 2. 默认宽高（可随层级缩放）
            grid = torch.stack([grid_x, grid_y], dim=-1)  # (H, W, 2)
            # grid = (grid.unsqueeze(0).expand(batch_size, -1, -1, -1) + 0.5) / scale
            # wh = torch.ones_like(grid) * 0.05 * (2.0**lvl)
            wh = torch.full_like(grid, 0.05 * (2.0 ** lvl))  # (H, W, 2)
            # 3. 【关键】添加角度（旋转框必须有！）
            angle = torch.zeros(H, W, 1, dtype=dtype, device=device)  # (H, W, 1)
            # proposal = torch.cat((grid, wh), -1).view(batch_size, -1, 4)
            # 4. 拼接成 5D: [cx, cy, w, h, angle]
            proposal_lvl = torch.cat([grid, wh, angle], dim=-1)  # (H, W, 5)
            # 5. 展平并扩展 batch
            proposal_lvl = proposal_lvl.view(1, -1, 5).expand(batch_size, -1, -1)
            assert_no_nan(proposal_lvl)
            proposals.append(proposal_lvl)
            # print(f"proposal: {proposal}")
            # proposals.append(proposal)
        output_proposals = torch.cat(proposals, 1)  # (B, total_tokens, 5)
        assert_no_nan(output_proposals)
        # do not use `all` to make it exportable to onnx
        # output_proposals_valid = (
        #     (output_proposals > 0.01) & (output_proposals < 0.99)).sum(
        #         -1, keepdim=True) == output_proposals.shape[-1]
        # 6. 有效区域 mask（只对 cxcywh 判断，angle 不参与）
        valid_cxcywh = (output_proposals[..., :4] > 0.01) & (output_proposals[..., :4] < 0.99)
        output_proposals_valid = valid_cxcywh.all(dim=-1, keepdim=True)  # (B, N, 1)
        assert_no_nan(output_proposals)
        # # inverse_sigmoid
        # output_proposals = torch.log(output_proposals / (1 - output_proposals))
        # output_proposals = output_proposals.masked_fill(
        #     ~output_proposals_valid, float('inf'))
        # 7. 对前 4 维做 inverse_sigmoid，angle 保持原值
        cxcywh = output_proposals[..., :4]
        angle = output_proposals[..., 4:5]

        cxcywh = torch.log(cxcywh / (1 - cxcywh))  # inverse sigmoid
        output_proposals = torch.cat([cxcywh, angle], dim=-1)
        assert_no_nan(output_proposals)

        # 8. 无效位置设为 inf
        output_proposals = output_proposals.masked_fill(~output_proposals_valid, float('inf'))
        assert_no_nan(output_proposals)
        return output_proposals.to(dtype), output_proposals_valid.to(dtype)